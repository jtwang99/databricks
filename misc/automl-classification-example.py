# Databricks notebook source
# MAGIC %md # AutoML classification example
# MAGIC 
# MAGIC ## Requirements
# MAGIC Databricks Runtime for Machine Learning 8.3 or above.<br>
# MAGIC [Databricks AutoML](https://docs.databricks.com/applications/machine-learning/automl.html?_ga=2.202522450.2135623742.1631839395-564988684.1622001878#databricks-automl)

# COMMAND ----------

# MAGIC %md ## Census income dataset
# MAGIC This dataset contains census data from the 1994 census database. Each row represents a group of individuals. The goal is to determine whether a group has an income of over 50k a year or not. This classification is represented as a string in the **income** column with values `<=50K` or `>50k`.

# COMMAND ----------

from pyspark.sql.types import DoubleType, StringType, StructType, StructField

schema = StructType([
  StructField("age", DoubleType(), False),
  StructField("workclass", StringType(), False),
  StructField("fnlwgt", DoubleType(), False),
  StructField("education", StringType(), False),
  StructField("education_num", DoubleType(), False),
  StructField("marital_status", StringType(), False),
  StructField("occupation", StringType(), False),
  StructField("relationship", StringType(), False),
  StructField("race", StringType(), False),
  StructField("sex", StringType(), False),
  StructField("capital_gain", DoubleType(), False),
  StructField("capital_loss", DoubleType(), False),
  StructField("hours_per_week", DoubleType(), False),
  StructField("native_country", StringType(), False),
  StructField("income", StringType(), False)
])
input_df = spark.read.format("csv").schema(schema).load("/databricks-datasets/adult/adult.data")

# COMMAND ----------

# MAGIC %md ## Train/test split

# COMMAND ----------

train_df, test_df = input_df.randomSplit([0.99, 0.01], seed=42)
display(train_df)

# COMMAND ----------

# MAGIC %md # Training
# MAGIC The following command starts an AutoML run. You must provide the column that the model should predict in the `target_col` argument.  
# MAGIC When the run completes, you can follow the link to the best trial notebook to examine the training code. This notebook also includes a feature importance plot.

# COMMAND ----------

from databricks import automl
summary = automl.classify(train_df, target_col="income", timeout_minutes=30)

# COMMAND ----------

# MAGIC %sh /databricks/python_shell/scripts/db_ipykernel_launcher.py configure

# COMMAND ----------

# MAGIC %md The following command displays information about the AutoML output. 

# COMMAND ----------

help(summary)

# COMMAND ----------

# MAGIC %md # Next steps
# MAGIC - Explore the notebooks and experiments linked above.
# MAGIC - If the metrics for the best trial notebook look good, skip directly to the inference section.
# MAGIC - If you want to improve on the model generated by the best trial:
# MAGIC   - Go to the notebook with the best trial and clone it.
# MAGIC   - Edit the notebook as necessary to improve the model. For example, you might try different hyperparameters.
# MAGIC   - When you are satisfied with the model, note the URI where the artifact for the trained model is logged. Assign this URI to the `model_uri` variable in Cmd 12.

# COMMAND ----------

# MAGIC %md
# MAGIC # Inference
# MAGIC 
# MAGIC You can use the model trained by AutoML to make predictions on new data. The examples below demonstrate how to make predictions on data in pandas DataFrames, or register the model as a Spark UDF for prediction on Spark DataFrames.

# COMMAND ----------

model_uri = summary.best_trial.model_path
# model_uri = "<model-uri-from-generated-notebook>"

# COMMAND ----------

# MAGIC %md ## pandas DataFrame

# COMMAND ----------

import mlflow

# Prepare test dataset
test_pdf = test_df.toPandas()
y_test = test_pdf["income"]
X_test = test_pdf.drop("income", axis=1)

# Run inference using the best model
model = mlflow.pyfunc.load_model(model_uri)
predictions = model.predict(X_test)
test_pdf["income_predicted"] = predictions
display(test_pdf)

# COMMAND ----------

# MAGIC %md ## Spark DataFrame

# COMMAND ----------

predict_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type="string")
display(test_df.withColumn("income_predicted", predict_udf()))

# COMMAND ----------

# MAGIC %md ## Test
# MAGIC Use the final model to make predictions on the holdout test set to estimate how the model would perform in a production setting. The diagram shows the breakdown between correct and incorrect predictions.

# COMMAND ----------

import sklearn.metrics

model = mlflow.sklearn.load_model(model_uri)
sklearn.metrics.plot_confusion_matrix(model, X_test, y_test)
